Skipping import of cpp extensions due to incompatible torch version 2.10.0a0+rocm7.11.0a20251210 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
WARNING 12-19 17:07:51 [attention.py:82] Using VLLM_V1_USE_PREFILL_DECODE_ATTENTION environment variable is deprecated and will be removed in v0.14.0 or v1.0.0, whichever is soonest. Please use --attention-config.use_prefill_decode_attention command line argument or AttentionConfig(use_prefill_decode_attention=...) config field instead.
[0;36m(APIServer pid=74952)[0;0m INFO 12-19 17:07:51 [api_server.py:1351] vLLM API server version 0.13.0rc2.dev112+g763963aa7.d20251213
[0;36m(APIServer pid=74952)[0;0m INFO 12-19 17:07:51 [utils.py:253] non-default args: {'model_tag': 'openai/gpt-oss-20b', 'host': '127.0.0.1', 'model': 'openai/gpt-oss-20b', 'trust_remote_code': True, 'max_model_len': 32768, 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.98, 'max_num_seqs': 64}
[0;36m(APIServer pid=74952)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[0;36m(APIServer pid=74952)[0;0m INFO 12-19 17:07:55 [model.py:514] Resolved architecture: GptOssForCausalLM
[0;36m(APIServer pid=74952)[0;0m Parse safetensors files:   0%|          | 0/3 [00:00<?, ?it/s]Parse safetensors files:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  6.81it/s]Parse safetensors files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 20.38it/s]
[0;36m(APIServer pid=74952)[0;0m INFO 12-19 17:07:55 [model.py:1636] Using max model len 32768
[0;36m(APIServer pid=74952)[0;0m INFO 12-19 17:07:56 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=74952)[0;0m INFO 12-19 17:07:56 [config.py:271] Overriding max cuda graph capture size to 1024 for performance.
Skipping import of cpp extensions due to incompatible torch version 2.10.0a0+rocm7.11.0a20251210 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
[0;36m(EngineCore_DP0 pid=75118)[0;0m INFO 12-19 17:08:00 [core.py:93] Initializing a V1 LLM engine (v0.13.0rc2.dev112+g763963aa7.d20251213) with config: model='openai/gpt-oss-20b', speculative_config=None, tokenizer='openai/gpt-oss-20b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=mxfp4, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='openai_gptoss', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=openai/gpt-oss-20b, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512, 528, 544, 560, 576, 592, 608, 624, 640, 656, 672, 688, 704, 720, 736, 752, 768, 784, 800, 816, 832, 848, 864, 880, 896, 912, 928, 944, 960, 976, 992, 1008, 1024], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 1024, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=75118)[0;0m WARNING 12-19 17:08:00 [multiproc_executor.py:884] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
Skipping import of cpp extensions due to incompatible torch version 2.10.0a0+rocm7.11.0a20251210 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Skipping import of cpp extensions due to incompatible torch version 2.10.0a0+rocm7.11.0a20251210 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
INFO 12-19 17:08:03 [parallel_state.py:1203] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:58633 backend=nccl
INFO 12-19 17:08:03 [parallel_state.py:1203] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:58633 backend=nccl
INFO 12-19 17:08:04 [pynccl.py:111] vLLM is using nccl==2.27.3
INFO 12-19 17:08:04 [parallel_state.py:1411] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
INFO 12-19 17:08:04 [parallel_state.py:1411] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(Worker_TP0 pid=75200)[0;0m INFO 12-19 17:08:05 [gpu_model_runner.py:3562] Starting to load model openai/gpt-oss-20b...
[0;36m(Worker_TP1 pid=75201)[0;0m INFO 12-19 17:08:05 [rocm.py:306] Using Rocm Attention backend on V1 engine.
[0;36m(Worker_TP1 pid=75201)[0;0m INFO 12-19 17:08:05 [layer.py:372] Enabled separate cuda stream for MoE shared_experts
[0;36m(Worker_TP1 pid=75201)[0;0m INFO 12-19 17:08:05 [mxfp4.py:170] Using Triton backend
[0;36m(Worker_TP0 pid=75200)[0;0m INFO 12-19 17:08:05 [rocm.py:306] Using Rocm Attention backend on V1 engine.
[0;36m(Worker_TP0 pid=75200)[0;0m INFO 12-19 17:08:05 [layer.py:372] Enabled separate cuda stream for MoE shared_experts
[0;36m(Worker_TP0 pid=75200)[0;0m INFO 12-19 17:08:05 [mxfp4.py:170] Using Triton backend
[0;36m(Worker_TP0 pid=75200)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
[0;36m(Worker_TP0 pid=75200)[0;0m Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:01,  1.12it/s]
[0;36m(Worker_TP0 pid=75200)[0;0m Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:01<00:00,  1.12it/s]
[0;36m(Worker_TP0 pid=75200)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:02<00:00,  1.14it/s]
[0;36m(Worker_TP0 pid=75200)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:02<00:00,  1.14it/s]
[0;36m(Worker_TP0 pid=75200)[0;0m 
[0;36m(Worker_TP0 pid=75200)[0;0m INFO 12-19 17:08:08 [default_loader.py:308] Loading weights took 2.68 seconds
[0;36m(Worker_TP0 pid=75200)[0;0m INFO 12-19 17:08:09 [gpu_model_runner.py:3659] Model loading took 7.4551 GiB memory and 3.520081 seconds
[0;36m(Worker_TP0 pid=75200)[0;0m INFO 12-19 17:08:11 [backends.py:634] Using cache directory: /home/kyuz0/.cache/vllm/torch_compile_cache/a6064e95d6/rank_0_0/backbone for vLLM's torch.compile
[0;36m(Worker_TP0 pid=75200)[0;0m INFO 12-19 17:08:11 [backends.py:694] Dynamo bytecode transform time: 1.83 s
[0;36m(Worker_TP1 pid=75201)[0;0m INFO 12-19 17:08:12 [backends.py:261] Cache the graph of compile range (1, 2048) for later use
[0;36m(Worker_TP0 pid=75200)[0;0m INFO 12-19 17:08:12 [backends.py:261] Cache the graph of compile range (1, 2048) for later use
[0;36m(Worker_TP0 pid=75200)[0;0m INFO 12-19 17:08:13 [backends.py:278] Compiling a graph for compile range (1, 2048) takes 1.26 s
[0;36m(Worker_TP0 pid=75200)[0;0m INFO 12-19 17:08:13 [monitor.py:34] torch.compile takes 3.09 s in total
[0;36m(Worker_TP0 pid=75200)[0;0m INFO 12-19 17:08:15 [gpu_worker.py:375] Available KV cache memory: 22.75 GiB
[0;36m(EngineCore_DP0 pid=75118)[0;0m INFO 12-19 17:08:15 [kv_cache_utils.py:1291] GPU KV cache size: 993,760 tokens
[0;36m(EngineCore_DP0 pid=75118)[0;0m INFO 12-19 17:08:15 [kv_cache_utils.py:1296] Maximum concurrency for 32,768 tokens per request: 56.85x
[0;36m(Worker_TP0 pid=75200)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/83 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   1%|          | 1/83 [00:00<00:30,  2.65it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|â–         | 2/83 [00:00<00:29,  2.71it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–Ž         | 3/83 [00:01<00:29,  2.74it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|â–         | 4/83 [00:01<00:28,  2.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 5/83 [00:01<00:28,  2.77it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   7%|â–‹         | 6/83 [00:02<00:27,  2.78it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 7/83 [00:02<00:27,  2.81it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|â–‰         | 8/83 [00:02<00:26,  2.82it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|â–ˆ         | 9/83 [00:03<00:26,  2.84it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 10/83 [00:03<00:25,  2.86it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  13%|â–ˆâ–Ž        | 11/83 [00:03<00:25,  2.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|â–ˆâ–        | 12/83 [00:04<00:24,  2.89it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 13/83 [00:04<00:23,  2.92it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|â–ˆâ–‹        | 14/83 [00:04<00:23,  2.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 15/83 [00:05<00:23,  2.95it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|â–ˆâ–‰        | 16/83 [00:05<00:22,  2.96it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–ˆ        | 17/83 [00:05<00:22,  2.96it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 18/83 [00:06<00:21,  2.98it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|â–ˆâ–ˆâ–Ž       | 19/83 [00:06<00:21,  2.97it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–       | 20/83 [00:06<00:21,  2.98it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–Œ       | 21/83 [00:07<00:20,  2.99it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 22/83 [00:07<00:20,  3.01it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|â–ˆâ–ˆâ–Š       | 23/83 [00:07<00:19,  3.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–‰       | 24/83 [00:08<00:19,  3.06it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  30%|â–ˆâ–ˆâ–ˆ       | 25/83 [00:08<00:18,  3.09it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 26/83 [00:08<00:18,  3.12it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 27/83 [00:09<00:17,  3.13it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 28/83 [00:09<00:17,  3.16it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|â–ˆâ–ˆâ–ˆâ–      | 29/83 [00:09<00:16,  3.19it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 30/83 [00:10<00:16,  3.22it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 31/83 [00:10<00:16,  3.24it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 32/83 [00:10<00:15,  3.26it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|â–ˆâ–ˆâ–ˆâ–‰      | 33/83 [00:11<00:15,  3.28it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 34/83 [00:11<00:14,  3.30it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 35/83 [00:11<00:14,  3.31it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 36/83 [00:11<00:14,  3.32it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 37/83 [00:12<00:13,  3.33it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 38/83 [00:12<00:13,  3.36it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 39/83 [00:12<00:13,  3.37it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 40/83 [00:13<00:12,  3.40it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 41/83 [00:13<00:12,  3.42it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 42/83 [00:13<00:11,  3.45it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 43/83 [00:13<00:11,  3.46it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 44/83 [00:14<00:11,  3.48it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 45/83 [00:14<00:10,  3.50it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 46/83 [00:14<00:10,  3.53it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 47/83 [00:15<00:10,  3.58it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 48/83 [00:15<00:09,  3.62it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 49/83 [00:15<00:09,  3.65it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 50/83 [00:15<00:09,  3.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 51/83 [00:16<00:08,  3.70it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 52/83 [00:16<00:08,  3.73it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 53/83 [00:16<00:08,  3.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 54/83 [00:16<00:07,  3.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 55/83 [00:17<00:07,  3.76it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 56/83 [00:17<00:07,  3.77it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 57/83 [00:17<00:06,  3.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 58/83 [00:17<00:06,  3.81it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 59/83 [00:18<00:06,  3.84it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 60/83 [00:18<00:05,  3.87it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 61/83 [00:18<00:05,  3.90it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 62/83 [00:18<00:05,  3.90it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 63/83 [00:19<00:05,  3.90it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 64/83 [00:19<00:04,  3.91it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 65/83 [00:19<00:04,  3.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 66/83 [00:20<00:04,  3.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 67/83 [00:20<00:04,  3.96it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 68/83 [00:20<00:03,  3.97it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 69/83 [00:20<00:03,  3.98it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 70/83 [00:21<00:03,  3.97it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 71/83 [00:21<00:03,  3.99it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 72/83 [00:21<00:02,  4.00it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 73/83 [00:21<00:02,  4.00it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 74/83 [00:21<00:02,  4.00it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 75/83 [00:22<00:01,  4.01it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 76/83 [00:22<00:01,  4.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 77/83 [00:22<00:01,  4.01it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 78/83 [00:22<00:01,  4.01it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 79/83 [00:23<00:00,  4.01it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 80/83 [00:23<00:00,  4.02it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 81/83 [00:23<00:00,  3.97it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 82/83 [00:24<00:00,  3.95it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83/83 [00:24<00:00,  3.96it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83/83 [00:24<00:00,  3.42it/s]
[0;36m(Worker_TP0 pid=75200)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   9%|â–‰         | 1/11 [00:00<00:02,  4.16it/s]Capturing CUDA graphs (decode, FULL):  18%|â–ˆâ–Š        | 2/11 [00:00<00:02,  4.39it/s]Capturing CUDA graphs (decode, FULL):  27%|â–ˆâ–ˆâ–‹       | 3/11 [00:00<00:01,  4.45it/s]Capturing CUDA graphs (decode, FULL):  36%|â–ˆâ–ˆâ–ˆâ–‹      | 4/11 [00:00<00:01,  4.45it/s]Capturing CUDA graphs (decode, FULL):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 5/11 [00:01<00:01,  4.44it/s]Capturing CUDA graphs (decode, FULL):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 6/11 [00:01<00:01,  4.44it/s]Capturing CUDA graphs (decode, FULL):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 7/11 [00:01<00:00,  4.44it/s]Capturing CUDA graphs (decode, FULL):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 8/11 [00:01<00:00,  4.44it/s]Capturing CUDA graphs (decode, FULL):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 9/11 [00:02<00:00,  4.41it/s]Capturing CUDA graphs (decode, FULL):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 10/11 [00:02<00:00,  4.40it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:02<00:00,  4.35it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:02<00:00,  4.40it/s]
[0;36m(Worker_TP0 pid=75200)[0;0m INFO 12-19 17:08:43 [gpu_model_runner.py:4610] Graph capturing finished in 27 secs, took 0.92 GiB
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] WorkerProc hit an exception.
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] Traceback (most recent call last):
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4302, in _dummy_sampler_run
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     sampler_output = self.sampler(
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]         logits=logits, sampling_metadata=dummy_metadata
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     )
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     return self._call_impl(*args, **kwargs)
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     return forward_call(*args, **kwargs)
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/sample/sampler.py", line 96, in forward
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]                                   ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/sample/sampler.py", line 187, in sample
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     random_sampled, processed_logprobs = self.topk_topp_sampler(
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]                                          ~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]         logits,
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]         ^^^^^^^
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     ...<2 lines>...
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]         sampling_metadata.top_p,
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]         ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     )
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     ^
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     return self._call_impl(*args, **kwargs)
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     return forward_call(*args, **kwargs)
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 104, in forward_native
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     logits = self.apply_top_k_top_p(logits, k, p)
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 258, in apply_top_k_top_p
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]                               ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] torch.OutOfMemoryError: HIP out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 31.86 GiB of which 0 bytes is free. Of the allocated memory 30.76 GiB is allocated by PyTorch, with 106.00 MiB allocated in private pools (e.g., HIP Graphs), and 135.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] 
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] The above exception was the direct cause of the following exception:
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] 
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] Traceback (most recent call last):
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/executor/multiproc_executor.py", line 821, in worker_busy_loop
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     output = func(*args, **kwargs)
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/worker/gpu_worker.py", line 538, in compile_or_warm_up_model
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     self.model_runner._dummy_sampler_run(hidden_states=last_hidden_states)
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/torch/utils/_contextlib.py", line 124, in decorate_context
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     return func(*args, **kwargs)
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4307, in _dummy_sampler_run
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     raise RuntimeError(
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     ...<4 lines>...
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     ) from e
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] RuntimeError: CUDA out of memory occurred when warming up sampler with 64 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] Traceback (most recent call last):
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4302, in _dummy_sampler_run
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] WorkerProc hit an exception.
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] Traceback (most recent call last):
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4302, in _dummy_sampler_run
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     sampler_output = self.sampler(
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]         logits=logits, sampling_metadata=dummy_metadata
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     )
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     return self._call_impl(*args, **kwargs)
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     return forward_call(*args, **kwargs)
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/sample/sampler.py", line 96, in forward
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]                                   ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/sample/sampler.py", line 187, in sample
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     random_sampled, processed_logprobs = self.topk_topp_sampler(
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]                                          ~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]         logits,
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]         ^^^^^^^
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     ...<2 lines>...
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]         sampling_metadata.top_p,
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]         ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     )
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     ^
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     return self._call_impl(*args, **kwargs)
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     return forward_call(*args, **kwargs)
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 104, in forward_native
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     logits = self.apply_top_k_top_p(logits, k, p)
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 258, in apply_top_k_top_p
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]                               ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] torch.OutOfMemoryError: HIP out of memory. Tried to allocate 150.00 MiB. GPU 1 has a total capacity of 31.86 GiB of which 0 bytes is free. Of the allocated memory 30.76 GiB is allocated by PyTorch, with 106.00 MiB allocated in private pools (e.g., HIP Graphs), and 119.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] 
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] The above exception was the direct cause of the following exception:
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] 
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] Traceback (most recent call last):
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/executor/multiproc_executor.py", line 821, in worker_busy_loop
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     output = func(*args, **kwargs)
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/worker/gpu_worker.py", line 538, in compile_or_warm_up_model
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     self.model_runner._dummy_sampler_run(hidden_states=last_hidden_states)
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/torch/utils/_contextlib.py", line 124, in decorate_context
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     return func(*args, **kwargs)
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4307, in _dummy_sampler_run
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     raise RuntimeError(
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     ...<4 lines>...
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     ) from e
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] RuntimeError: CUDA out of memory occurred when warming up sampler with 64 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] Traceback (most recent call last):
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4302, in _dummy_sampler_run
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     sampler_output = self.sampler(
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]         logits=logits, sampling_metadata=dummy_metadata
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     )
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     return self._call_impl(*args, **kwargs)
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     return forward_call(*args, **kwargs)
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/sample/sampler.py", line 96, in forward
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]                                   ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/sample/sampler.py", line 187, in sample
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     random_sampled, processed_logprobs = self.topk_topp_sampler(
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]                                          ~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]         logits,
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]         ^^^^^^^
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     ...<2 lines>...
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]         sampling_metadata.top_p,
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]         ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     )
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     ^
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     return self._call_impl(*args, **kwargs)
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     return forward_call(*args, **kwargs)
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 104, in forward_native
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     logits = self.apply_top_k_top_p(logits, k, p)
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 258, in apply_top_k_top_p
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]                               ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] torch.OutOfMemoryError: HIP out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 31.86 GiB of which 0 bytes is free. Of the allocated memory 30.76 GiB is allocated by PyTorch, with 106.00 MiB allocated in private pools (e.g., HIP Graphs), and 135.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] 
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] The above exception was the direct cause of the following exception:
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] 
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] Traceback (most recent call last):
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/executor/multiproc_executor.py", line 821, in worker_busy_loop
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     output = func(*args, **kwargs)
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/worker/gpu_worker.py", line 538, in compile_or_warm_up_model
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     self.model_runner._dummy_sampler_run(hidden_states=last_hidden_states)
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/torch/utils/_contextlib.py", line 124, in decorate_context
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     return func(*args, **kwargs)
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4307, in _dummy_sampler_run
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     raise RuntimeError(
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     ...<4 lines>...
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     ) from e
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] RuntimeError: CUDA out of memory occurred when warming up sampler with 64 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
[0;36m(Worker_TP0 pid=75200)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] 
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     sampler_output = self.sampler(
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]         logits=logits, sampling_metadata=dummy_metadata
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     )
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     return self._call_impl(*args, **kwargs)
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     return forward_call(*args, **kwargs)
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/sample/sampler.py", line 96, in forward
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]                                   ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/sample/sampler.py", line 187, in sample
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     random_sampled, processed_logprobs = self.topk_topp_sampler(
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]                                          ~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]         logits,
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]         ^^^^^^^
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     ...<2 lines>...
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]         sampling_metadata.top_p,
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]         ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     )
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     ^
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     return self._call_impl(*args, **kwargs)
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     return forward_call(*args, **kwargs)
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 104, in forward_native
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     logits = self.apply_top_k_top_p(logits, k, p)
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 258, in apply_top_k_top_p
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]                               ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] torch.OutOfMemoryError: HIP out of memory. Tried to allocate 150.00 MiB. GPU 1 has a total capacity of 31.86 GiB of which 0 bytes is free. Of the allocated memory 30.76 GiB is allocated by PyTorch, with 106.00 MiB allocated in private pools (e.g., HIP Graphs), and 119.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] 
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] The above exception was the direct cause of the following exception:
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] 
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] Traceback (most recent call last):
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/executor/multiproc_executor.py", line 821, in worker_busy_loop
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     output = func(*args, **kwargs)
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/worker/gpu_worker.py", line 538, in compile_or_warm_up_model
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     self.model_runner._dummy_sampler_run(hidden_states=last_hidden_states)
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/torch/utils/_contextlib.py", line 124, in decorate_context
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     return func(*args, **kwargs)
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4307, in _dummy_sampler_run
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     raise RuntimeError(
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     ...<4 lines>...
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826]     ) from e
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] RuntimeError: CUDA out of memory occurred when warming up sampler with 64 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
[0;36m(Worker_TP1 pid=75201)[0;0m ERROR 12-19 17:08:43 [multiproc_executor.py:826] 
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]         vllm_config, executor_class, log_stats, executor_fail_callback
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]     )
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]     ^
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]         vllm_config
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]     )
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]     ^
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]     self.model_executor.initialize_from_config(kv_cache_configs)
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]     self.collective_rpc("compile_or_warm_up_model")
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/executor/multiproc_executor.py", line 361, in collective_rpc
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]     return aggregate(get_response())
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]                      ~~~~~~~~~~~~^^
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/executor/multiproc_executor.py", line 344, in get_response
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]     raise RuntimeError(
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]     ...<2 lines>...
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866]     )
[0;36m(EngineCore_DP0 pid=75118)[0;0m ERROR 12-19 17:08:43 [core.py:866] RuntimeError: Worker failed with error 'CUDA out of memory occurred when warming up sampler with 64 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.', please check the stack trace above for the root cause
[0;36m(EngineCore_DP0 pid=75118)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=75118)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=75118)[0;0m   File "/usr/lib64/python3.13/multiprocessing/process.py", line 313, in _bootstrap
[0;36m(EngineCore_DP0 pid=75118)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=75118)[0;0m     ~~~~~~~~^^
[0;36m(EngineCore_DP0 pid=75118)[0;0m   File "/usr/lib64/python3.13/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=75118)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=75118)[0;0m     ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=75118)[0;0m   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=75118)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=75118)[0;0m   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=75118)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=75118)[0;0m   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=75118)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=75118)[0;0m     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=75118)[0;0m         vllm_config, executor_class, log_stats, executor_fail_callback
[0;36m(EngineCore_DP0 pid=75118)[0;0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=75118)[0;0m     )
[0;36m(EngineCore_DP0 pid=75118)[0;0m     ^
[0;36m(EngineCore_DP0 pid=75118)[0;0m   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=75118)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=75118)[0;0m                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=75118)[0;0m         vllm_config
[0;36m(EngineCore_DP0 pid=75118)[0;0m         ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=75118)[0;0m     )
[0;36m(EngineCore_DP0 pid=75118)[0;0m     ^
[0;36m(EngineCore_DP0 pid=75118)[0;0m   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=75118)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[0;36m(EngineCore_DP0 pid=75118)[0;0m     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=75118)[0;0m   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[0;36m(EngineCore_DP0 pid=75118)[0;0m     self.collective_rpc("compile_or_warm_up_model")
[0;36m(EngineCore_DP0 pid=75118)[0;0m     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=75118)[0;0m   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/executor/multiproc_executor.py", line 361, in collective_rpc
[0;36m(EngineCore_DP0 pid=75118)[0;0m     return aggregate(get_response())
[0;36m(EngineCore_DP0 pid=75118)[0;0m                      ~~~~~~~~~~~~^^
[0;36m(EngineCore_DP0 pid=75118)[0;0m   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/executor/multiproc_executor.py", line 344, in get_response
[0;36m(EngineCore_DP0 pid=75118)[0;0m     raise RuntimeError(
[0;36m(EngineCore_DP0 pid=75118)[0;0m     ...<2 lines>...
[0;36m(EngineCore_DP0 pid=75118)[0;0m     )
[0;36m(EngineCore_DP0 pid=75118)[0;0m RuntimeError: Worker failed with error 'CUDA out of memory occurred when warming up sampler with 64 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.', please check the stack trace above for the root cause
[0;36m(Worker_TP0 pid=75200)[0;0m INFO 12-19 17:08:43 [multiproc_executor.py:711] Parent process exited, terminating worker
[0;36m(Worker_TP1 pid=75201)[0;0m INFO 12-19 17:08:43 [multiproc_executor.py:711] Parent process exited, terminating worker
[0;36m(APIServer pid=74952)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=74952)[0;0m   File "/opt/venv/bin/vllm", line 7, in <module>
[0;36m(APIServer pid=74952)[0;0m     sys.exit(main())
[0;36m(APIServer pid=74952)[0;0m              ~~~~^^
[0;36m(APIServer pid=74952)[0;0m   File "/opt/venv/lib64/python3.13/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[0;36m(APIServer pid=74952)[0;0m     args.dispatch_function(args)
[0;36m(APIServer pid=74952)[0;0m     ~~~~~~~~~~~~~~~~~~~~~~^^^^^^
[0;36m(APIServer pid=74952)[0;0m   File "/opt/venv/lib64/python3.13/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[0;36m(APIServer pid=74952)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=74952)[0;0m     ~~~~~~~~~~^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=74952)[0;0m   File "/opt/venv/lib64/python3.13/site-packages/uvloop/__init__.py", line 96, in run
[0;36m(APIServer pid=74952)[0;0m     return __asyncio.run(
[0;36m(APIServer pid=74952)[0;0m            ~~~~~~~~~~~~~^
[0;36m(APIServer pid=74952)[0;0m         wrapper(),
[0;36m(APIServer pid=74952)[0;0m         ^^^^^^^^^^
[0;36m(APIServer pid=74952)[0;0m     ...<2 lines>...
[0;36m(APIServer pid=74952)[0;0m         **run_kwargs
[0;36m(APIServer pid=74952)[0;0m         ^^^^^^^^^^^^
[0;36m(APIServer pid=74952)[0;0m     )
[0;36m(APIServer pid=74952)[0;0m     ^
[0;36m(APIServer pid=74952)[0;0m   File "/usr/lib64/python3.13/asyncio/runners.py", line 195, in run
[0;36m(APIServer pid=74952)[0;0m     return runner.run(main)
[0;36m(APIServer pid=74952)[0;0m            ~~~~~~~~~~^^^^^^
[0;36m(APIServer pid=74952)[0;0m   File "/usr/lib64/python3.13/asyncio/runners.py", line 118, in run
[0;36m(APIServer pid=74952)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=74952)[0;0m            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
[0;36m(APIServer pid=74952)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=74952)[0;0m   File "/opt/venv/lib64/python3.13/site-packages/uvloop/__init__.py", line 48, in wrapper
[0;36m(APIServer pid=74952)[0;0m     return await main
[0;36m(APIServer pid=74952)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=74952)[0;0m   File "/opt/venv/lib64/python3.13/site-packages/vllm/entrypoints/openai/api_server.py", line 1398, in run_server
[0;36m(APIServer pid=74952)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=74952)[0;0m   File "/opt/venv/lib64/python3.13/site-packages/vllm/entrypoints/openai/api_server.py", line 1417, in run_server_worker
[0;36m(APIServer pid=74952)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=74952)[0;0m                ~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(APIServer pid=74952)[0;0m         args,
[0;36m(APIServer pid=74952)[0;0m         ^^^^^
[0;36m(APIServer pid=74952)[0;0m         client_config=client_config,
[0;36m(APIServer pid=74952)[0;0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=74952)[0;0m     ) as engine_client:
[0;36m(APIServer pid=74952)[0;0m     ^
[0;36m(APIServer pid=74952)[0;0m   File "/usr/lib64/python3.13/contextlib.py", line 214, in __aenter__
[0;36m(APIServer pid=74952)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=74952)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=74952)[0;0m   File "/opt/venv/lib64/python3.13/site-packages/vllm/entrypoints/openai/api_server.py", line 172, in build_async_engine_client
[0;36m(APIServer pid=74952)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=74952)[0;0m                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(APIServer pid=74952)[0;0m         engine_args,
[0;36m(APIServer pid=74952)[0;0m         ^^^^^^^^^^^^
[0;36m(APIServer pid=74952)[0;0m     ...<2 lines>...
[0;36m(APIServer pid=74952)[0;0m         client_config=client_config,
[0;36m(APIServer pid=74952)[0;0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=74952)[0;0m     ) as engine:
[0;36m(APIServer pid=74952)[0;0m     ^
[0;36m(APIServer pid=74952)[0;0m   File "/usr/lib64/python3.13/contextlib.py", line 214, in __aenter__
[0;36m(APIServer pid=74952)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=74952)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=74952)[0;0m   File "/opt/venv/lib64/python3.13/site-packages/vllm/entrypoints/openai/api_server.py", line 213, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=74952)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=74952)[0;0m         vllm_config=vllm_config,
[0;36m(APIServer pid=74952)[0;0m     ...<6 lines>...
[0;36m(APIServer pid=74952)[0;0m         client_index=client_index,
[0;36m(APIServer pid=74952)[0;0m     )
[0;36m(APIServer pid=74952)[0;0m   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/engine/async_llm.py", line 215, in from_vllm_config
[0;36m(APIServer pid=74952)[0;0m     return cls(
[0;36m(APIServer pid=74952)[0;0m         vllm_config=vllm_config,
[0;36m(APIServer pid=74952)[0;0m     ...<9 lines>...
[0;36m(APIServer pid=74952)[0;0m         client_index=client_index,
[0;36m(APIServer pid=74952)[0;0m     )
[0;36m(APIServer pid=74952)[0;0m   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/engine/async_llm.py", line 134, in __init__
[0;36m(APIServer pid=74952)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=74952)[0;0m                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(APIServer pid=74952)[0;0m         vllm_config=vllm_config,
[0;36m(APIServer pid=74952)[0;0m         ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=74952)[0;0m     ...<4 lines>...
[0;36m(APIServer pid=74952)[0;0m         client_index=client_index,
[0;36m(APIServer pid=74952)[0;0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=74952)[0;0m     )
[0;36m(APIServer pid=74952)[0;0m     ^
[0;36m(APIServer pid=74952)[0;0m   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
[0;36m(APIServer pid=74952)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=74952)[0;0m   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/engine/core_client.py", line 820, in __init__
[0;36m(APIServer pid=74952)[0;0m     super().__init__(
[0;36m(APIServer pid=74952)[0;0m     ~~~~~~~~~~~~~~~~^
[0;36m(APIServer pid=74952)[0;0m         asyncio_mode=True,
[0;36m(APIServer pid=74952)[0;0m         ^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=74952)[0;0m     ...<3 lines>...
[0;36m(APIServer pid=74952)[0;0m         client_addresses=client_addresses,
[0;36m(APIServer pid=74952)[0;0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=74952)[0;0m     )
[0;36m(APIServer pid=74952)[0;0m     ^
[0;36m(APIServer pid=74952)[0;0m   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/engine/core_client.py", line 477, in __init__
[0;36m(APIServer pid=74952)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=74952)[0;0m          ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=74952)[0;0m   File "/usr/lib64/python3.13/contextlib.py", line 148, in __exit__
[0;36m(APIServer pid=74952)[0;0m     next(self.gen)
[0;36m(APIServer pid=74952)[0;0m     ~~~~^^^^^^^^^^
[0;36m(APIServer pid=74952)[0;0m   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/engine/utils.py", line 903, in launch_core_engines
[0;36m(APIServer pid=74952)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=74952)[0;0m     ~~~~~~~~~~~~~~~~~~~~~~~^
[0;36m(APIServer pid=74952)[0;0m         handshake_socket,
[0;36m(APIServer pid=74952)[0;0m         ^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=74952)[0;0m     ...<5 lines>...
[0;36m(APIServer pid=74952)[0;0m         coordinator.proc if coordinator else None,
[0;36m(APIServer pid=74952)[0;0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=74952)[0;0m     )
[0;36m(APIServer pid=74952)[0;0m     ^
[0;36m(APIServer pid=74952)[0;0m   File "/opt/venv/lib64/python3.13/site-packages/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
[0;36m(APIServer pid=74952)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=74952)[0;0m     ...<3 lines>...
[0;36m(APIServer pid=74952)[0;0m     )
[0;36m(APIServer pid=74952)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
